{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del(DataCleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import import_ipynb\n",
    "\n",
    "from Source import DataCleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_zip_file=\"./Datasets/hate-speech-and-offensive-language-master.zip\"\n",
    "# directory_to_extract_to=\"./Datasets\"\n",
    "# with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
    "#     zip_ref.extractall(directory_to_extract_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./Datasets/hate-speech-and-offensive-language-master/data/labeled_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`count` = number of CrowdFlower users who coded each tweet (min is 3, sometimes more users coded a tweet when judgments were determined to be unreliable by CF).\n",
    "\n",
    "`hate_speech` = number of CF users who judged the tweet to be hate speech.\n",
    "\n",
    "`offensive_language` = number of CF users who judged the tweet to be offensive.\n",
    "\n",
    "`neither` = number of CF users who judged the tweet to be neither offensive nor non-offensive.\n",
    "\n",
    "`class` = class label for majority of CF users.\n",
    "  0 - hate speech\n",
    "  1 - offensive  language\n",
    "  2 - neither"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth= 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=\"@mayasolovely:\"\n",
    "print(re.sub('[^a-zA-Zr\"\\'\\\"\"\\d\"\"]+','',x))\n",
    "print(re.sub(re.compile('@\\S+'),'',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Processed tweet']=copy.deepcopy(data['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove @Usernames\n",
    "data['Processed tweet']= data['Processed tweet'].map(lambda x:re.sub(re.compile('@\\S+'),'',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data['Processed tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas.api.types import is_numeric_dtype\n",
    "# from pandas.api.types import is_string_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=re.sub('[^a-zA-Zr\"\\'\\\"\"\\d\"\"]+',' ',data['tweet'][0])\n",
    "y.replace(\"'\",'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DataCleaning.preProcessColumn(data['tweet'],fillna=True,removeNumbers=True,\n",
    "#                      removeSpecialCharacters=True,handleCamelCase=False,\n",
    "#                     removeMailids=False,removeUrls=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data=DataCleaning.preProcessData(data=data,textColumns=[\"Processed tweet\"],customStopWords=[\"rt\",\"amp\"],\n",
    "                            fillna=True,removeNumbers=True,\n",
    "                            removeSpecialCharacters=True,handleCamelCase=False,\n",
    "                            removeStopwords=True,removeMailids=True,removeUrls=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# x=\"http://t.co/H0dYEBvnZB\"\n",
    "# re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',' ',x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['Processed tweet']=data['Processed tweet'].apply(DataCleaning.lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print,sdg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Processed tweet'].apply(DataCleaning.lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['Processed tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"./Data/dataProcessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from subprocess import check_output\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['tweet_without_stopwords'] = test['tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = data['Processed tweet'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "doc=[len(d) for d in sentences]\n",
    "plt.hist(doc,bins=100)\n",
    "plt.title(\"Distribution of tweets Word length\")\n",
    "plt.xlabel(\"Tweets Word length\")\n",
    "plt.ylabel(\"No of Tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "mpl.rcParams['figure.figsize']=(12,12)\n",
    "mpl.rcParams['font.size']=12\n",
    "mpl.rcParams['savefig.dpi']=120\n",
    "mpl.rcParams['figure.subplot.bottom']=0.1\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "background_color='white',\n",
    "max_words=200,\n",
    "max_font_size=40,\n",
    "random_state=42).generate(str(sentences))\n",
    "\n",
    "print(wordcloud)\n",
    "fig=plt.figure(1)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigrams\n",
    "\n",
    "def topnBigrams(corpus,n=10):\n",
    "    vec= CountVectorizer(ngram_range=(2,2),stop_words={'english'}).fit(corpus)\n",
    "    bow= vec.transform(corpus)\n",
    "    sumWords= bow.sum(axis==0)\n",
    "    wordFreq= [(word,sumWords[0,idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    wordFreq = sorted(wordFreq,key=lambda x:x[1], reverse=True)\n",
    "    \n",
    "    return wordFreq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# vec= CountVectorizer(ngram_range=(2,2))\n",
    "# bow= vec.transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topBigrams= topnBigrams(sentences,20)\n",
    "df= pd.DataFrame(topBigrams,columns=['bigram','count'])\n",
    "\n",
    "fig= go.Figure([go.Bar(x=df['bigram'],y=df['count'])])\n",
    "fig.update_layout(title=go.layout.Title(Text=\"Top 20 bigrams\"))\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
